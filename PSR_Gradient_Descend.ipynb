{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSR_Gradient_descend_method(cost_function, init_theta_list, args, learning_rate, iterations):\n",
    "    #get gradient of the cost function by Parameter Shift Rule\n",
    "    def get_psr_gradient(cost_function, params, args):\n",
    "\n",
    "        gradient = []\n",
    "        for i in range(len(params)):\n",
    "\n",
    "            shifted_params_plus = []\n",
    "            shifted_params_minus = []\n",
    "            for element in params:\n",
    "                shifted_params_plus.append(element)\n",
    "                shifted_params_minus.append(element)\n",
    "\n",
    "            shifted_params_plus[i] = params[i] + np.pi/2\n",
    "            shifted_params_minus[i] = params[i] - np.pi/2\n",
    "\n",
    "            cost_function_plus = cost_function(shifted_params_plus, *args)\n",
    "            cost_function_minus = cost_function(shifted_params_minus, *args)\n",
    "\n",
    "            gradient.append(0.5*(cost_function_plus - cost_function_minus))\n",
    "\n",
    "        return gradient\n",
    "\n",
    "        #get the new set of parameters knowing the gradient and the learning rate\n",
    "    def get_new_params(params, gradient, learning_rate):\n",
    "\n",
    "        return [params[i] - learning_rate * gradient[i] for i in range(len(params)) ]\n",
    "\n",
    "    #iterates the process\n",
    "    f_list = []\n",
    "    theta_list = init_theta_list\n",
    "    gradient_list = []\n",
    "    for j in range(iterations):\n",
    "\n",
    "        gradient = get_psr_gradient(cost_function, theta_list, args)\n",
    "        theta_list = get_new_params(theta_list, gradient, learning_rate)\n",
    "\n",
    "        f_list.append(cost_function(theta_list, *args )) \n",
    "        gradient_list.append(gradient)\n",
    "    return f_list, gradient_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSR_Gradient_Descend:\n",
    "    def __init__(self, cost_function, init_theta_list, args, learning_rate = 0.01, iterations = 50, tol = 0.001 ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        self.f_list, self.gradient = PSR_Gradient_descend_method(cost_function, init_theta_list, args, learning_rate, iterations)\n",
    "        \n",
    "            \n",
    "    def get_fun(self, num_of_values):\n",
    "        return self.f_list[len(self.f_list) - num_of_values :]\n",
    "    \n",
    "    def get_gradient(self, num_of_values):\n",
    "        return self.gradient[len(self.gradient) - num_of_values :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
